import logging
import os
from src.domain.models import ProcessingResult, LegalChunk # Import type

logger = logging.getLogger(__name__)

class IndexingPipeline:
    def __init__(self, loader, chunker, embedder, db):
        self.loader = loader
        self.chunker = chunker
        self.embedder = embedder
        self.db = db

    def run_pipeline(self, file_path: str):
        filename = os.path.basename(file_path)
        logger.info(f" B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file: {filename}")

        try:
            # 1. Load
            raw_text = self.loader.load(file_path)
            if not raw_text:
                raise Exception("File r·ªóng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c text")

            # 2. Chunk
            chunks = self.chunker.chunk(raw_text, filename)
            logger.info(f"‚úÇÔ∏è ƒê√£ c·∫Øt th√†nh {len(chunks)} chunks.")

            # 3. Embed & Validate Metadata (BATCH PROCESSING)
            vectors = []
            valid_chunks = []
            
            # Chu·∫©n b·ªã list texts
            texts_to_embed = []
            chunks_to_embed = []
            
            for chunk in chunks:
                text_content = chunk.text if hasattr(chunk, 'text') else chunk.get('text', '')
                if text_content and len(text_content.strip()) > 0:
                    texts_to_embed.append(text_content)
                    chunks_to_embed.append(chunk)

            if texts_to_embed:
                logger.info(f"üöÄ ƒêang g·ª≠i {len(texts_to_embed)} chunks t·ªõi Embedding Service (Batch Mode)...")
                try:
                    # G·ªçi Batch API 1 l·∫ßn duy nh·∫•t
                    batch_vectors = self.embedder.get_embeddings_batch(texts_to_embed)
                    
                    if len(batch_vectors) == len(chunks_to_embed):
                        vectors = batch_vectors
                        valid_chunks = chunks_to_embed
                        logger.info(f"‚úÖ ƒê√£ nh·∫≠n ƒë∆∞·ª£c {len(vectors)} vectors.")
                    else:
                        logger.error(f"‚ùå L·ªói: S·ªë l∆∞·ª£ng vector tr·∫£ v·ªÅ ({len(batch_vectors)}) kh√¥ng kh·ªõp s·ªë l∆∞·ª£ng chunk ({len(chunks_to_embed)})")
                
                except Exception as e:
                    logger.error(f"‚ùå L·ªói Batch Embedding: {e}")

            # 4. Save Batch
            if valid_chunks:
                # H√†m save_chunks c·ªßa DB adapter c·∫ßn x·ª≠ l√Ω vi·ªác map metadata t·ª´ chunk v√†o Weaviate properties
                self.db.save_chunks(valid_chunks, vectors)
                logger.info(f" ƒê√£ l∆∞u th√†nh c√¥ng {len(valid_chunks)} chunks c√≥ metadata v√†o Weaviate.")
            else:
                logger.warning(" Kh√¥ng c√≥ chunk n√†o h·ª£p l·ªá ƒë·ªÉ l∆∞u.")

            return ProcessingResult(
                filename=filename,
                status="success",
                total_chunks=len(valid_chunks),
                message="Indexing completed successfully"
            )

        except Exception as e:
            logger.error(f" L·ªói Pipeline Fatal: {str(e)}", exc_info=True)
            return ProcessingResult(
                filename=filename,
                status="error",
                message=str(e),
                total_chunks=0
            )