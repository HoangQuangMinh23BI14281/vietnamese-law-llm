import streamlit as st
import requests
import os

# --- C·∫§U H√åNH ---
# D√πng rstrip('/') ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªã th·ª´a d·∫•u / ·ªü cu·ªëi URL
LLM_GATEWAY = os.getenv("LLM_GATEWAY_URL", "http://llm-gateway:8001").rstrip('/')
INDEXING_SERVICE = os.getenv("INDEXING_SERVICE_URL", "http://indexing-service:5000").rstrip('/')

st.set_page_config(page_title="Vietnam Legal AI", page_icon="‚öñÔ∏è", layout="wide")

st.title("‚öñÔ∏è H·ªá th·ªëng T∆∞ v·∫•n Ph√°p lu·∫≠t Th√¥ng minh")
st.markdown("---")

# T·∫°o Tabs
tab1, tab2 = st.tabs(["üìö N·∫°p Ki·∫øn Th·ª©c (Indexing)", "ü§ñ H·ªèi ƒê√°p Ph√°p L√Ω (Chat)"])

# ==========================================
# TAB 1: N·∫†P D·ªÆ LI·ªÜU
# ==========================================
with tab1:
    st.header("N·∫°p vƒÉn b·∫£n lu·∫≠t m·ªõi")
    st.info("üí° H·ªá th·ªëng s·ª≠ d·ª•ng Docling ƒë·ªÉ ƒë·ªçc PDF (gi·ªØ b·∫£ng bi·ªÉu) v√† Neo4j ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã li√™n k·∫øt.")
    
    # Th√™m key ƒë·ªÉ c√≥ th·ªÉ reset uploader n·∫øu c·∫ßn
    uploaded_file = st.file_uploader("T·∫£i file PDF lu·∫≠t (V√≠ d·ª•: Lu·∫≠t ƒê·∫•t ƒëai 2024)", type=['pdf'])
    
    if uploaded_file and st.button("üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω", type="primary"):
        with st.spinner("‚è≥ ƒêang g·ª≠i file sang Indexing Service (vui l√≤ng ƒë·ª£i)..."):
            try:
                # [FIX 1] D√πng .getvalue() ƒë·ªÉ l·∫•y bytes an to√†n
                files = {
                    "file": (uploaded_file.name, uploaded_file.getvalue(), "application/pdf")
                }
                
                # [FIX 2] URL n·ªëi chu·∫©n
                api_url = f"{INDEXING_SERVICE}/process-file-upload"
                
                # Timeout 600s (10 ph√∫t) v√¨ Docling x·ª≠ l√Ω OCR kh√° l√¢u
                res = requests.post(api_url, files=files, timeout=600)
                
                if res.status_code == 200:
                    st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong! File: {uploaded_file.name}")
                    # Hi·ªÉn th·ªã k·∫øt qu·∫£ JSON ƒë·∫πp h∆°n
                    with st.expander("Xem chi ti·∫øt k·∫øt qu·∫£ x·ª≠ l√Ω"):
                        st.json(res.json())
                else:
                    st.error(f"‚ùå L·ªói Server ({res.status_code}): {res.text}")
                    
            except requests.exceptions.ConnectionError:
                st.error(f"üîå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Indexing Service t·∫°i: {INDEXING_SERVICE}")
                st.caption("G·ª£i √Ω: Ki·ªÉm tra xem container 'indexing-service' c√≥ ƒëang ch·∫°y kh√¥ng?")
            except Exception as e:
                st.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")

# ==========================================
# TAB 2: CHAT BOT
# ==========================================
with tab2:
    st.header("Tr·ª£ l√Ω Lu·∫≠t s∆∞ AI")
    
    # 1. Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # 2. Render l·∫°i l·ªãch s·ª≠ chat c≈©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 3. X·ª≠ l√Ω Input m·ªõi
    if prompt := st.chat_input("H√£y h·ªèi v·ªÅ lu·∫≠t (VD: ƒêi·ªÅu ki·ªán t√°ch th·ª≠a ƒë·∫•t ·ªü H√† N·ªôi?)"):
        # Hi·ªÉn th·ªã c√¢u h·ªèi User ngay l·∫≠p t·ª©c
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # G·ªçi Backend
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("üîÑ _ƒêang tra c·ª©u Vector DB v√† Knowledge Graph..._")
            
            try:
                api_url = f"{LLM_GATEWAY}/chat"
                res = requests.post(api_url, json={"query": prompt}, timeout=60)
                
                if res.status_code == 200:
                    data = res.json()
                    answer = data.get("answer", "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.")
                    sources = data.get("sources", [])
                    
                    # Format c√¢u tr·∫£ l·ªùi k√®m ngu·ªìn 
                    full_response = answer
                    if sources:
                        full_response += "\n\n---\n**üìö Ngu·ªìn tham kh·∫£o:**\n" + "\n".join([f"- {s}" for s in sources])
                    
                    # C·∫≠p nh·∫≠t UI
                    message_placeholder.markdown(full_response)
                    
                    # L∆∞u v√†o Session State
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                else:
                    error_msg = f"‚ö†Ô∏è L·ªói t·ª´ h·ªá th·ªëng AI ({res.status_code})"
                    message_placeholder.error(error_msg)
            
            except requests.exceptions.ConnectionError:
                message_placeholder.error(f"üîå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi LLM Gateway t·∫°i: {LLM_GATEWAY}")
            except Exception as e:
                message_placeholder.error(f"‚ùå L·ªói k·∫øt n·ªëi: {e}")