# src/application/chat_service.py
from typing import List, Optional
import logging
import re
from src.domain.models import ChatQuery, ChatResponse, RetrievedDocument
from src.domain.ports import EmbeddingPort, VectorDBPort, LLMPort

logger = logging.getLogger(__name__)

class ChatService:
    def __init__(self, embedder: EmbeddingPort, vector_db: VectorDBPort, llm: LLMPort):
        self.embedder = embedder
        self.vector_db = vector_db
        self.llm = llm
        
        self.article_pattern = re.compile(
            r"\b(?:ƒëi·ªÅu|kho·∫£n)\s+(\d+)\b(?!\s*(?:nƒÉm|th√°ng|ng√†y|gi·ªù|ph√∫t|tri·ªáu|t·ª∑|ngh√¨n|trƒÉm|ƒë·ªìng|vnd|usd))", 
            re.IGNORECASE
        )

    def process_question(self, req: ChatQuery) -> ChatResponse:
        logger.info(f"üöÄ [CRAG Start] C√¢u h·ªèi: {req.query}")
        
        # --- B∆Ø·ªöC 1: ROUTING & INITIAL RETRIEVAL ---
        article_match = self.article_pattern.search(req.query)
        docs = []
        search_mode = "semantic"
        
        if article_match:
            # CASE A: T√¨m ch√≠nh x√°c
            article_num = article_match.group(1)
            target = f"ƒêi·ªÅu {article_num}"
            logger.info(f" Ph√°t hi·ªán √Ω ƒë·ªãnh t√¨m c·ª• th·ªÉ: {target}")
            
            docs = self.vector_db.search(
                query_text=target,
                vector=self.embedder.get_embedding(target),
                limit=5,
                where_filter={
                    "path": ["article"],
                    "operator": "Equal",
                    "valueString": target
                }
            )
            search_mode = "strict"
        else:
            # CASE B: T√¨m ng·ªØ nghƒ©a
            logger.info(" T√¨m ki·∫øm ng·ªØ nghƒ©a r·ªông (Broad Search)...")
            docs = self.vector_db.search(
                query_text=req.query,
                vector=self.embedder.get_embedding(req.query),
                limit=8,
                alpha=0.5
            )

        # --- B∆Ø·ªöC 2: GRADING (CH·∫§M ƒêI·ªÇM) ---
        is_relevant = self._grade_documents(req.query, docs, search_mode)
        
        # --- B∆Ø·ªöC 3: CORRECTIVE ACTIONS (S·ª¨A SAI) ---
        if not is_relevant:
            logger.warning(f" [Correction] K·∫øt qu·∫£ t·ª´ ch·∫ø ƒë·ªô '{search_mode}' KH√îNG T·ªêT.")
            
            if search_mode == "strict":
                logger.info(" Chuy·ªÉn sang Broad Search (B·ªè filter ƒêi·ªÅu)...")
                docs = self.vector_db.search(
                    query_text=req.query,
                    vector=self.embedder.get_embedding(req.query),
                    limit=8,
                    alpha=0.5
                )
                if not self._grade_documents(req.query, docs, "semantic"):
                    logger.info(" Broad Search v·∫´n ch∆∞a t·ªët -> K√≠ch ho·∫°t HyDE...")
                    docs = self._run_hyde_search(req.query)
            else:
                logger.info(" Semantic Search th·∫•t b·∫°i -> K√≠ch ho·∫°t HyDE...")
                docs = self._run_hyde_search(req.query)

        # --- B∆Ø·ªöC 4: FINAL GENERATION ---
        return self._generate_final_response(req.query, docs)

    # ==========================================
    # C√ÅC H√ÄM PH·ª§ TR·ª¢ (HELPER METHODS)
    # ==========================================

    def _grade_documents(self, query: str, docs: List, mode: str) -> bool:
        if not docs: 
            return False
            
        top_doc = docs[0].content[:500]
        
        # Prompt ch·∫•m ƒëi·ªÉm (Kh√¥ng c·∫ßn s·ª≠a nhi·ªÅu, ch·ªâ c·∫ßn YES/NO)
        sys_prompt = "You are a Relevance Grader. Output only YES or NO."
        
        prompt_logic = ""
        if mode == "strict":
            prompt_logic = "If the query is about time duration (e.g., '5 years') but the document is a Law Article 'Article 5', output NO."
        
        user_prompt = f"""
        Query: "{query}"
        Document: "{top_doc}..."
        
        {prompt_logic}
        
        Does the document help answer the query? 
        Answer exclusively with: YES or NO.
        """
        
        try:
            grade = self.llm.generate_answer(sys_prompt, user_prompt).strip().upper()
            logger.info(f" Grader ({mode}): {grade}")
            return "YES" in grade
        except:
            return True

    def _run_hyde_search(self, query: str):
        hyde_doc = self._generate_hyde_doc(query)
        logger.info(f" HyDE Document generated: {hyde_doc[:50]}...")
        hyde_vector = self.embedder.get_embedding(hyde_doc)
        return self.vector_db.search(
            query_text=hyde_doc,
            vector=hyde_vector,
            limit=10,
            alpha=0.7
        )

    def _generate_hyde_doc(self, query: str) -> str:
        # Prompt gi·∫£ ƒë·ªãnh
        sys_prompt = "B·∫°n l√† chuy√™n gia lu·∫≠t Vi·ªát Nam."
        user_prompt = f"Vi·∫øt m·ªôt ƒëo·∫°n vƒÉn ng·∫Øn gi·∫£ ƒë·ªãnh (b·∫±ng ti·∫øng Vi·ªát) c√≥ ch·ª©a c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi: {query}"
        return self.llm.generate_answer(sys_prompt, user_prompt)

    def _generate_final_response(self, query: str, docs: List) -> ChatResponse:
        if not docs:
            return ChatResponse(answer="Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√°p l√Ω li√™n quan trong c∆° s·ªü d·ªØ li·ªáu.", sources=[])
            
        # T·∫°o context string g·ªçn g√†ng h∆°n
        context_str = "\n".join([f"- {d.title}: {d.content}" for d in docs])
        sources = list(set([d.title for d in docs]))
        
        sys_prompt = "B·∫°n l√† tr·ª£ l√Ω lu·∫≠t s∆∞ Vi·ªát Nam. Nhi·ªám v·ª• duy nh·∫•t c·ªßa b·∫°n l√† tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát."
        
        # Nh√©t y√™u c·∫ßu Ti·∫øng Vi·ªát xu·ªëng cu·ªëi c√πng (Recency Bias - Model nh·ªõ c√°i cu·ªëi t·ªët h∆°n)
        user_prompt = f"""
        T√ÄI LI·ªÜU THAM KH·∫¢O:
        {context_str}
        
        C√ÇU H·ªéI: "{query}"
        
        Y√äU C·∫¶U NGHI√äM NG·∫∂T:
        1. D·ª±a v√†o t√†i li·ªáu tr√™n ƒë·ªÉ tr·∫£ l·ªùi.
        2. Sau khi suy nghƒ© xong, C√ÇU TR·∫¢ L·ªúI CU·ªêI C√ôNG PH·∫¢I VI·∫æT B·∫∞NG TI·∫æNG VI·ªÜT.
        3. Kh√¥ng ƒë∆∞·ª£c vi·∫øt ti·∫øng Anh ·ªü k·∫øt qu·∫£ cu·ªëi c√πng.
        
        H√ÉY TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT NGAY D∆Ø·ªöI ƒê√ÇY:
        """
        
        # G·ªçi model
        answer = self.llm.generate_answer(sys_prompt, user_prompt)
        return ChatResponse(answer=answer, sources=sources)