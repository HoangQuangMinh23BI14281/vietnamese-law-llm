# src/infrastructure/llm_adapter.py
import logging
import torch
import re  # <--- Quan tr·ªçng: D√πng ƒë·ªÉ x·ª≠ l√Ω chu·ªói
from transformers import AutoModelForCausalLM, AutoTokenizer
from src.domain.ports import LLMPort

logger = logging.getLogger(__name__)

class QwenLocalAdapter(LLMPort):
    def __init__(self, model_name: str = "Qwen/Qwen3-0.6B"):
        logger.info(f"‚è≥ ƒêang t·∫£i model {model_name}...")
        self.model_name = model_name
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True 
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        logger.info(f"‚úÖ ƒê√£ t·∫£i xong model Qwen 3")

    def generate_answer(self, system_prompt: str, user_prompt: str) -> str:
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            # B·∫≠t ch·∫ø ƒë·ªô Thinking
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=True 
            )
            
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                generated_ids = self.model.generate(
                    **model_inputs,
                    max_new_tokens=32768, 
                    temperature=0.6,
                    top_p=0.95
                )

            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
            raw_content = self.tokenizer.decode(output_ids, skip_special_tokens=True)
            
            # --- X·ª¨ L√ù PH·∫¶N THINKING ---
            
            # 1. In ph·∫ßn suy nghƒ© ra LOG ƒë·ªÉ b·∫°n theo d√µi (Debug)
            think_match = re.search(r'<think>(.*?)</think>', raw_content, re.DOTALL)
            if think_match:
                thinking_process = think_match.group(1).strip()
                logger.info(f"üß† [Model Thinking]: {thinking_process[:200]}...") # Ch·ªâ log 200 k√Ω t·ª± ƒë·∫ßu cho g·ªçn
            
            # 2. X√≥a ph·∫ßn <think> ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi s·∫°ch
            clean_content = re.sub(r'<think>.*?</think>', '', raw_content, flags=re.DOTALL).strip()
            
            return clean_content

        except Exception as e:
            logger.error(f"‚ùå Qwen 3 Error: {e}")
            return "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë x·ª≠ l√Ω."